{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cddbadca",
   "metadata": {},
   "source": [
    "Name : Pranav Bilwanikar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3074689",
   "metadata": {},
   "source": [
    "<h1 align = \"center\">Chapter - 2</h1>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8857757",
   "metadata": {},
   "source": [
    "**Question 1.** Write a python program to find out the words after '@' from the below sentences with the use of regex.\n",
    "\n",
    "\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4967526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmail.com\n",
      "yahoo.com\n",
      "hotmail.com\n",
      "ineuron.ai\n",
      "outlook.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = [\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\"]\n",
    "\n",
    "\n",
    "for te in text:\n",
    "    print(re.findall(r'[\\w\\.-]+', te)[1])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c06657d",
   "metadata": {},
   "source": [
    "**Question 2.** Write a python program with the use of regex to take out the word \"New\" from the following sentence.\n",
    "\n",
    "[\"New Delhi is the capital of India\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15625975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New']\n"
     ]
    }
   ],
   "source": [
    "text = \"New Delhi is the capital of India\"\n",
    "print(re.findall('New', text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a51784d8",
   "metadata": {},
   "source": [
    "**Question 3.** Create one python program in which you have to lowercase the sentence first and than delete digits from the following sentence.\n",
    "\n",
    "\"In India, 184 people got affected with Corona virus and 4 are died.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28f1082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowered Text:  in india, 184 people got affected with corona virus and 4 are died.\n",
      "in india      people got affected with corona virus and   are died \n"
     ]
    }
   ],
   "source": [
    "text = \"In India, 184 people got affected with Corona virus and 4 are died.\"\n",
    "\n",
    "text = text.lower()\n",
    "print(\"Lowered Text: \", text)\n",
    "print(re.sub(r'[^a-z]',' ', text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4959e43",
   "metadata": {},
   "source": [
    "**Question 4.** Do stemming, lemmatization and tokenization from the following sentence.\n",
    "\n",
    "\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89910f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "ps = PorterStemmer()\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e86742",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I hope that, when I have built up my savings, I will be able to travel to Hawai.\"\n",
    "sent = nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e75a17a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenizer:  ['I', 'hope', 'that', ',', 'when', 'I', 'have', 'built', 'up', 'my', 'savings', ',', 'I', 'will', 'be', 'able', 'to', 'travel', 'to', 'Hawai', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Word Tokenizer: \", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e742a6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentStemming = [ps.stem(word) for word in sent if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b7a6492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Stemming after removing stopwords:  ['i', 'hope', ',', 'i', 'built', 'save', ',', 'i', 'abl', 'travel', 'hawai', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Words Stemming after removing stopwords: \", sentStemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b59a6754",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized words after removing stopwords are:  ['I', 'hope', ',', 'I', 'built', 'saving', ',', 'I', 'able', 'travel', 'Hawai', '.']\n"
     ]
    }
   ],
   "source": [
    "wordLemme = [wordnet.lemmatize(word) for word in sent if word not in stopwords.words('english')]\n",
    "print(\"Lemmatized words after removing stopwords are: \", wordLemme)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8f3e902",
   "metadata": {},
   "source": [
    "**Question 5.** Create one python program from the following sentence.\n",
    "\n",
    "\"I love NLP, not you\"\n",
    "\n",
    "output : ['I', 'l', 'N', 'n', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b77cb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP', 'not', 'you']\n",
      "\n",
      "Desired Output: \n",
      "['I', 'l', 'N', 'n', 'y']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "text = \"I love NLP, not you\"\n",
    "token = RegexpTokenizer(r'\\w+')\n",
    "wordTok = token.tokenize(text)\n",
    "print(wordTok)\n",
    "\n",
    "wordFirst = []\n",
    "for word in wordTok:\n",
    "    wordFirst.append(word[0])\n",
    "    \n",
    "print(\"\\nDesired Output: \")\n",
    "print(wordFirst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
